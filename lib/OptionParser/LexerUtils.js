// Construct a token.
//
// type - e.g. 'UNDERSCORE'
// token - e.g. '_'
// pos - the (starting) position in the string where it occurred
export function Token(type, token, pos) {
  return { type, token, pos };
}

// Construct a response returned by a lexer.
//
// tokens - an array of tokens generated by the lexer; may be empty
// newCharacterStream - a new character stream for the next lexer
function LexerResponse(tokens, newCharacterStream) {
  return { tokens, newCharacterStream };
}

// A simple "stream" wrapper around an array or string.
//
// Input:
//
//   buffer - the underlying array/string
//   pos - the 'zero' index of the stream
//
// Properties:
//
//   length - the length of the buffer remaining from index pos
//   present - whether the above length is not zero
//   empty - negation of the above
//   rest() - the buffer sliced from pos onward
//   get() - the item in the buffer at pos
//   advance(index = 1) - advance the stream forward by `index` characters;
//                        returns a new Stream
//   take(n) - return the next `n` items in the stream (or as many as are left,
//             whichever is greater)
//
// The calling code can pretend they're just dealing with the slice, but we
// keep track of where we are in the underlying list.

function Stream(buffer, pos = 0) {
  return {
    buffer,
    pos,
    length: buffer.length - pos,
    present: pos < buffer.length,
    empty: pos >= buffer.length,
    rest: () => buffer.slice(pos),
    get: () => buffer[pos],
    advance: (index = 1) => Stream(buffer, pos + index),
    take: (n) => buffer.slice(pos, pos + n)
  }
}

function CharacterStream(fullString, pos = 0) {
  return {
    ...Stream(fullString, pos),
    advance: (index = 1) => CharacterStream(fullString, pos + index),
    flush: () => CharacterStream(fullString, fullString.length),
    Token: (type, token) => Token(type, token, pos),
  }
}

export function TokenStream(buffer, pos = 0) {
  return {
    ...Stream(buffer, pos),
    advance: (index = 1) => TokenStream(buffer, pos + index),
    ofType: (type) => (pos < buffer.length && buffer[pos].type === type),
  }
}

// Define a tokenizer matching what's left in the stream with a regex. A `^` is
// automatically prepended to the regex, so there is no need to include it
// yourself.
//
// Example:
//
//   const WORD = regex('WORD', /\S+\s*/);
//   Lexer(WORD)('this is a string')
//   // => [
//     Token('WORD',  'this ',    0),
//     Token('WORD',  'is ',      5),
//     Token('WORD',  'a ',       8),
//     Token('WORD',  'string ',  10)
//   ]
//   
export function regex(type, regex, flags = '') {
  var massagedRegex = new RegExp(/^/.source + regex.source, flags);

  return function(previousTokens, charStream) {
    var match;
    if (match = charStream.rest().match(massagedRegex)) {
      return LexerResponse(
        [charStream.Token(type, match[0])],
        charStream.advance(match[0].length)
      );
    } else {
      return null;
    }
  }
}

// Like the regex matcher, but throws away the matched token.
export function skip(baseMatcher) {
  return function(previousTokens, charStream) {
    var match;
    if (match = baseMatcher(previousTokens, charStream)) {
      return LexerResponse([], match.newCharacterStream);
    } else {
      return null;
    }
  }
}

export function optional(matcher) {
  return function(previousTokens, charStream) {
    var match;
    if (match = matcher(previousTokens, charStream)) {
      return LexerResponse([], match.newCharacterStream);
    } else {
      return LexerResponse([], charStream);
    }
  }
}

function seq2(first, second) {
  return function(previousTokens, charStream) {
    const firstMatch = first(previousTokens, charStream);

    if (!firstMatch) {
      return null;
    }

    const secondMatch =
      second([...previousTokens, ...firstMatch.tokens], firstMatch.newCharacterStream);

    if (!secondMatch) {
      return null;
    }

    return LexerResponse(
      [...firstMatch.tokens, ...secondMatch.tokens],
      secondMatch.newCharacterStream
    );
  }
}

export function seq(firstMatcher, secondMatcher, thirdMatcher, ...rest) {
  const _seq2 = seq2(firstMatcher, secondMatcher);

  if (thirdMatcher) {
    return seq(_seq2, thirdMatcher, ...rest);
  } else {
    return _seq2;
  }
}

export function precededByToken(type) {
  return function(previousTokens, charStream) {
    const lastToken = previousTokens[previousTokens.length - 1];
    if (lastToken && lastToken.type == type) {
      return LexerResponse([], charStream);
    } else {
      return null;
    }
  }
}

export function map(fn, matcher) {
  return function(previousTokens, charStream) {
    var match;
    if (match = matcher(previousTokens, charStream)) {
      const mappedTokens = match.tokens.map(
        ({ type, token, pos }) => Token(type, fn(token), pos));

      return LexerResponse(
        mappedTokens, match.newCharacterStream);
    } else {
      return null;
    }
  }
}

export function or(...matchers) {
  return function(previousTokens, charStream) {
    var match;
    for (var i = 0; i < matchers.length; i++) {
      if (match = matchers[i](previousTokens, charStream)) {
        return match;
      }
    }

    return null;
  }
}

export function repeat(matcher) {
  return function(previousTokens, charStream) {
    var tokens = [];
    var counter = 0;

    while (charStream.present) {
      var match = matcher(tokens, charStream);

      if (!match) {
        break;
      }

      tokens = [...tokens, ...match.tokens];

      if (counter++ > 10000) {
        throw "tried to lex more than 10,000 tokens - this is probably a bug.";
      }

      charStream = match.newCharacterStream;
    }

    return LexerResponse(tokens, charStream);
  }
}

export function Lexer(_lexer) {
  return function(str) {
    var charStream = CharacterStream(str);
    var matcher = repeat(or(_lexer, regex('UNKNOWN', /.*/)));

    return matcher([], charStream).tokens;
  }
}
